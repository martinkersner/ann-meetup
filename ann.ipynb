{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Martin Kersner\n",
    "\n",
    "Inspired by http://neuralnetworksanddeeplearning.com/ and https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stochastic gradient descent learning\n",
    "* backpropagation\n",
    "* forward pass\n",
    "* backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Perceptron model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Image of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO MATRIX MULTIPLICATION EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE AND INITIALIZE\n",
    "\n",
    "# The first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron.  \n",
    "layers = [784, 30, 10]\n",
    "\n",
    "# Altogether 3 layers.\n",
    "num_layers = len(layers)\n",
    "\n",
    "# The biases and weights for the network are initialized randomly, using a Gaussian distribution\n",
    "# with mean 0, and variance 1. The first layer is assumed to be an input layer, and by convention we\n",
    "# won't set any biases for those neurons.\n",
    "biases  = [np.random.randn(y, 1) for y in layers[1:]]\n",
    "weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# BIASES in layers\n",
    "for b in biases:\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.30503735],\n",
      "       [-2.0172162 ],\n",
      "       [ 2.34590152],\n",
      "       [ 0.04772411],\n",
      "       [ 0.10493059],\n",
      "       [-0.17695414],\n",
      "       [ 0.06039735],\n",
      "       [-0.74968976],\n",
      "       [ 2.48688299],\n",
      "       [-0.51129384],\n",
      "       [-0.19949587],\n",
      "       [ 0.96690361],\n",
      "       [-0.38192341],\n",
      "       [-0.35852068],\n",
      "       [ 0.14755971],\n",
      "       [-0.0583262 ],\n",
      "       [ 0.6104622 ],\n",
      "       [-0.20084729],\n",
      "       [ 1.2773627 ],\n",
      "       [ 2.08316059],\n",
      "       [-1.24937959],\n",
      "       [ 0.40998501],\n",
      "       [-0.21876898],\n",
      "       [ 0.03384332],\n",
      "       [ 0.48363108],\n",
      "       [-1.17309328],\n",
      "       [-1.06886546],\n",
      "       [ 1.20950807],\n",
      "       [ 0.5837781 ],\n",
      "       [ 0.13228863]]), array([[ 0.27174564],\n",
      "       [ 0.08313185],\n",
      "       [-0.81254148],\n",
      "       [-0.92382359],\n",
      "       [-1.79719705],\n",
      "       [-0.19825696],\n",
      "       [-0.49012204],\n",
      "       [-0.77018402],\n",
      "       [ 0.63274159],\n",
      "       [-0.73313625]])]\n"
     ]
    }
   ],
   "source": [
    "# BIAS VALUES\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 784)\n",
      "(10, 30)\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS in layers\n",
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.36632624, -1.82993888,  1.01955074, ..., -0.97742111,\n",
      "         0.81002345, -0.32401526],\n",
      "       [-1.00501302,  0.50313174, -1.01357183, ..., -0.38885141,\n",
      "        -1.68692214,  0.00250013],\n",
      "       [ 0.49744882,  0.6660339 , -0.86823573, ..., -0.93251747,\n",
      "         0.16668425, -0.83623839],\n",
      "       ..., \n",
      "       [-0.18011558,  0.11147221, -1.35659312, ...,  0.25778556,\n",
      "         0.4993568 ,  0.40536535],\n",
      "       [-1.34483849,  1.44326691, -0.10079735, ...,  1.14333084,\n",
      "        -1.17620073, -0.03287093],\n",
      "       [ 1.09937851,  0.9849584 ,  1.04583451, ...,  0.34685665,\n",
      "         0.54461601, -0.71468065]]), array([[ -7.24690099e-01,   1.58134612e-01,   1.47351283e+00,\n",
      "         -6.87015998e-01,   9.68725674e-01,   4.26436716e-01,\n",
      "         -1.09860804e+00,   6.78812977e-03,   2.41013252e+00,\n",
      "         -1.19315224e+00,  -4.63145234e-01,   1.19493056e+00,\n",
      "         -2.99866422e-01,   1.86283831e-01,   2.97704491e-01,\n",
      "          1.38136127e+00,   9.60814384e-02,   1.60228911e+00,\n",
      "         -1.06067674e+00,  -9.54854459e-01,  -1.72444031e+00,\n",
      "          2.95358110e-01,  -5.08519703e-01,   7.14122953e-01,\n",
      "          3.12667949e-02,  -4.88105166e-01,  -7.36293517e-01,\n",
      "         -2.18858440e+00,   1.56863709e-02,   9.03412678e-01],\n",
      "       [ -5.63472963e-01,  -3.45452443e-01,   1.60164252e-02,\n",
      "          3.13914333e-01,   1.45375126e+00,   9.70819444e-01,\n",
      "         -1.37184328e+00,  -1.66887055e-01,   3.75827510e-01,\n",
      "         -1.00759491e+00,   3.58427148e-01,  -1.75256947e+00,\n",
      "          1.30210655e+00,  -4.69206423e-01,  -3.70570863e-01,\n",
      "         -3.59515387e-01,   9.38660715e-03,  -1.10162757e+00,\n",
      "         -8.73210346e-01,  -1.42976933e+00,  -5.95257802e-01,\n",
      "         -1.57116949e+00,  -1.74057861e+00,   6.88600678e-01,\n",
      "         -4.72045261e-01,   1.11039967e+00,   3.71047960e-01,\n",
      "         -1.03268934e+00,  -1.86869919e+00,   1.08287810e+00],\n",
      "       [  1.73850756e+00,   1.39714328e+00,  -2.55288687e-01,\n",
      "          8.41554664e-01,  -1.27806939e+00,  -4.74105134e-01,\n",
      "         -1.35889015e+00,  -1.34441664e-01,   1.42858772e+00,\n",
      "          3.39086828e-01,  -2.48964150e+00,  -2.81952176e+00,\n",
      "          3.89141330e-01,  -4.53817949e-01,  -1.29965220e+00,\n",
      "         -2.37593857e-01,  -2.14707029e+00,  -1.90714886e-01,\n",
      "          1.52157365e+00,  -5.56091734e-01,  -7.96631267e-01,\n",
      "          1.79797407e-01,   1.00837814e+00,  -1.36292479e+00,\n",
      "          8.27785091e-01,  -9.39521788e-01,  -5.96741957e-01,\n",
      "         -1.78370972e+00,   9.69174844e-01,  -6.91076076e-01],\n",
      "       [ -8.18469040e-01,  -8.12960804e-02,   1.22772829e+00,\n",
      "         -8.89363137e-01,   9.61142234e-01,   7.03347914e-02,\n",
      "          1.02813001e+00,   8.95262108e-02,  -1.10802804e-01,\n",
      "          1.87972389e-01,  -7.73394956e-01,   6.07094678e-02,\n",
      "         -7.11347525e-01,   6.23880939e-01,   8.24113622e-01,\n",
      "          9.43891834e-02,  -1.48451071e+00,   6.42989924e-02,\n",
      "         -1.10316289e+00,  -1.05461270e-01,  -3.41558626e-01,\n",
      "          3.76084457e-01,   4.64189717e-01,  -1.85476923e+00,\n",
      "          4.10507274e-01,   2.80361883e-01,   1.32726366e+00,\n",
      "          1.19481447e+00,  -2.00304690e-01,  -2.74671534e-01],\n",
      "       [ -1.18474534e+00,  -3.40312807e-01,  -2.11434495e-01,\n",
      "         -2.61003618e-01,   5.14087217e-01,   6.97088430e-01,\n",
      "          7.17957887e-01,   4.13042824e-02,  -2.14316238e-01,\n",
      "         -1.06025518e+00,  -8.76719319e-01,   1.06363637e+00,\n",
      "         -3.89199595e-01,   3.89273345e-02,   1.22666739e+00,\n",
      "         -7.30174958e-01,   1.54432676e+00,   6.60746650e-01,\n",
      "          1.71950085e+00,  -1.48920596e-01,  -9.31165037e-02,\n",
      "          1.69386681e+00,  -7.15075602e-01,  -2.92933407e-01,\n",
      "         -3.15484531e-01,  -5.76640252e-02,   1.62934077e-01,\n",
      "         -1.29313536e+00,  -4.81951220e-01,  -6.04806424e-02],\n",
      "       [ -8.12339555e-01,   7.98314779e-01,  -3.78011903e-01,\n",
      "         -4.23188125e-01,   1.38708212e+00,   5.60310090e-01,\n",
      "         -1.16522321e+00,  -6.66038311e-01,  -2.53822631e-01,\n",
      "         -1.09336558e+00,  -1.93657029e-04,   1.84595768e+00,\n",
      "         -4.19757281e-01,  -7.56774073e-01,   6.99073082e-01,\n",
      "         -8.87812356e-01,   2.27781770e-02,  -5.37184043e-02,\n",
      "          1.06011823e+00,  -5.50833028e-01,  -1.18848867e-01,\n",
      "          5.90162187e-01,  -1.12579550e+00,   3.95501786e-01,\n",
      "         -5.71368306e-01,  -1.17100168e+00,  -6.58585477e-01,\n",
      "         -1.27032522e+00,  -2.54920365e-01,   8.74579954e-01],\n",
      "       [  1.02119352e+00,  -1.99802626e+00,  -2.05681613e-01,\n",
      "          1.02070712e+00,   1.76911149e+00,  -1.10838165e+00,\n",
      "          2.86030357e-01,   1.52129359e+00,  -2.38110151e+00,\n",
      "          1.50550200e+00,  -8.53848118e-01,   8.76626929e-02,\n",
      "          1.75465032e+00,   8.84422146e-01,   7.09137222e-02,\n",
      "         -1.54713643e+00,  -9.89052396e-01,  -9.83078526e-01,\n",
      "          8.26821802e-01,   1.52152111e+00,  -4.92659522e-01,\n",
      "         -9.90865327e-01,  -6.56524393e-01,   4.21980986e-01,\n",
      "          7.18113857e-01,  -1.21511311e+00,  -1.42061860e+00,\n",
      "          1.49650200e+00,   2.48197214e+00,  -6.40746113e-01],\n",
      "       [ -1.53684440e+00,  -1.17483824e+00,  -1.57708176e+00,\n",
      "          2.01258657e+00,   1.79747673e-01,  -2.29394862e+00,\n",
      "          2.31870463e-01,  -1.33937223e-01,   1.09004824e-02,\n",
      "          6.25490464e-01,   6.05855980e-01,   6.27385796e-01,\n",
      "          1.90037676e+00,  -5.80071901e-01,   1.43718949e+00,\n",
      "          4.21929286e-01,   5.07352722e-02,   5.83102971e-01,\n",
      "         -5.31922293e-02,  -9.15199868e-01,  -1.04964479e-02,\n",
      "          2.24494488e-02,  -9.00642888e-01,  -6.73014825e-01,\n",
      "          9.16781283e-01,   9.47573553e-01,  -1.91310976e+00,\n",
      "         -1.65686921e+00,  -8.71741542e-01,   8.92042924e-01],\n",
      "       [ -4.13488038e-02,   1.49457315e+00,  -2.33827790e-01,\n",
      "         -3.01549795e-01,  -1.84933523e-01,  -1.20461724e+00,\n",
      "         -1.24335661e+00,   1.43793473e+00,  -1.21729998e-01,\n",
      "         -9.89725335e-01,   3.11482676e-01,   4.56490447e-01,\n",
      "         -5.35435337e-01,  -2.69722514e-01,  -6.40943532e-01,\n",
      "          1.93307225e-02,   7.10794356e-01,   1.43592273e+00,\n",
      "          5.60319263e-01,  -9.80290609e-01,  -1.01261060e+00,\n",
      "          3.35520426e-01,   5.81749570e-01,  -5.42860447e-01,\n",
      "          8.87324142e-01,  -4.56244478e-01,  -6.19976373e-01,\n",
      "         -5.58873127e-01,  -3.53520006e-01,   1.45181004e+00],\n",
      "       [  1.58754241e-01,   2.60710497e-01,  -1.00482755e+00,\n",
      "         -7.22988656e-01,   2.04723307e-01,  -6.43453885e-01,\n",
      "         -1.01513818e+00,  -1.97084619e-01,  -8.54363117e-01,\n",
      "          4.23848517e-01,   2.73434519e+00,  -1.17327369e-01,\n",
      "          7.86067895e-01,  -3.92044350e-01,   4.69045481e-01,\n",
      "          1.68072888e+00,   1.19211422e-01,   1.00428358e+00,\n",
      "          5.89076020e-01,  -2.09937364e+00,  -1.59072483e+00,\n",
      "         -2.04712339e+00,  -4.18873747e-01,   1.55393344e+00,\n",
      "         -9.75804912e-01,  -1.41359472e+00,  -9.43632278e-01,\n",
      "          8.74733859e-01,   1.13811518e+00,  -5.16815227e-01]])]\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation function\n",
    "* introduces non-linearity\n",
    "\n",
    "<img src=\"files/sigmoid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(biases, weights, a):\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "### Derivation of sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def cost_derivative(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations.\"\"\"\n",
    "    return (output_activations-y)\n",
    "        \n",
    "def backprop(biases, weights, x, y):\n",
    "    \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "    gradient for the cost function C_x.  ``nabla_b`` and\n",
    "    ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    \n",
    "    nabla_b = zeros_like(biases)\n",
    "    nabla_w = zeros_like(weights)\n",
    "    \n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    # backward pass\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    # Note that the variable l in the loop below is used a little\n",
    "    # differently to the notation in Chapter 2 of the book.  Here,\n",
    "    # l = 1 means the last layer of neurons, l = 2 is the\n",
    "    # second-last layer, and so on.  It's a renumbering of the\n",
    "    # scheme in the book, used here to take advantage of the fact\n",
    "    # that Python can use negative indices in lists.\n",
    "    num_layers = len(weights)+1\n",
    "    for l in xrange(2, num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch\n",
    "* TODO minibatch explanation\n",
    "* TOOD update update explanation\n",
    "* TODO eta explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eta - learning rate\n",
    "def update_mini_batch(biases, weights, mini_batch, eta):\n",
    "    def update(v, nv, eta, mini_batch_size):\n",
    "        return v-(eta/mini_batch_size)*nv\n",
    "    \n",
    "    def cumulate(nabla_v, delta_nabla_v):\n",
    "        return [nv+dnv for nv, dnv in zip(nabla_v, delta_nabla_v)]\n",
    "        \n",
    "    nabla_b = zeros_like(biases)\n",
    "    nabla_w = zeros_like(weights)\n",
    "    \n",
    "    # !! TODO explanation\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = backprop(biases, weights, x, y)\n",
    "        nabla_b = cumulate(nabla_b, delta_nabla_b)\n",
    "        nabla_w = cumulate(nabla_w, delta_nabla_w)\n",
    "    \n",
    "    mini_batch_size = len(mini_batch)\n",
    "    \n",
    "    weights = [update(w, nw, eta, mini_batch_size)\n",
    "               for w, nw in zip(weights, nabla_w)]\n",
    "    \n",
    "    biases = [update(b, nb, eta, mini_batch_size)\n",
    "              for b, nb in zip(biases, nabla_b)]\n",
    "    \n",
    "    return biases, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stochastic gradient descent\n",
    "* Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(biases, weights, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \"\"\"Train the neural network using mini-batch stochastic\n",
    "    gradient descent.  The ``training_data`` is a list of tuples\n",
    "    ``(x, y)`` representing the training inputs and the desired\n",
    "    outputs.  The other non-optional parameters are\n",
    "    self-explanatory.  If ``test_data`` is provided then the\n",
    "    network will be evaluated against the test data after each\n",
    "    epoch, and partial progress printed out.  This is useful for\n",
    "    tracking progress, but slows things down substantially.\"\"\"\n",
    "    \n",
    "    if test_data:\n",
    "        n_test = len(test_data)\n",
    "        \n",
    "    n = len(training_data)\n",
    "    test_acc_lst = []\n",
    "    train_acc_lst = []\n",
    "    \n",
    "    for j in xrange(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        \n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            biases, weights = update_mini_batch(biases, weights, mini_batch, eta)\n",
    "            \n",
    "        if test_data:\n",
    "            test_acc = evaluate(biases, weights, test_data)\n",
    "            test_acc_lst.append(test_acc/n_test)\n",
    "            \n",
    "            train_acc = evaluate(biases, weights, training_data)\n",
    "            train_acc_lst.append(train_acc/n)\n",
    "            \n",
    "            print(\"Epoch {0}: {1} / {2}\".format(j, test_acc, n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "            \n",
    "    return biases, weights, train_acc_lst, test_acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(biases, weights, test_data):\n",
    "    \"\"\"Return the number of test inputs for which the neural\n",
    "    network outputs the correct result. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    \n",
    "    test_results = [(np.argmax(feedforward(biases, weights, x)), y) for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "* http://yann.lecun.com/exdb/mnist/\n",
    "* 28 by 28 pixels\n",
    "* grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADGlJREFUeJzt3X+o3fV9x/HnW9f+YSyoa3YN1nm7IgtBmMpVJobR4awu\nKLH/SEQhY3U3SIQV9sfE/TFhDMpYO/aHFFKMTUdnO4jBWMa0k2o6GCFR6q9oq5NoE6JJTLEW/+g0\n7/1xv+mu8Z7vuTm/vufm/XzA4Z7zfZ/z/b455JXP93u+33M+kZlIquecrhuQ1A3DLxVl+KWiDL9U\nlOGXijL8UlGGXyrK8EtFGX6pqN+a5MYiwssJpTHLzFjO84Ya+SPi5oj4aUS8HhH3DbMuSZMVg17b\nHxHnAj8DbgQOAfuAOzLzQMtrHPmlMZvEyH8t8HpmvpGZvwa+B2wcYn2SJmiY8F8C/HzR40PNso+J\niPmI2B8R+4fYlqQRG/sHfpm5DdgG7vZL02SYkf8wcOmix59rlklaAYYJ/z7g8oj4fER8GtgE7B5N\nW5LGbeDd/sz8MCLuBZ4AzgW2Z+bLI+tM0lgNfKpvoI15zC+N3UQu8pG0chl+qSjDLxVl+KWiDL9U\nlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/\nVNREp+hWPevWretZu+6661pfe8stt7TW5+fnW+vHjh1rrVfnyC8VZfilogy/VJThl4oy/FJRhl8q\nyvBLRQ11nj8iDgLvAx8BH2bm3Cia0tmj7Vz81q1bh1r31Vdf3Vp/4oknhlr/2W4UF/n8cWYeH8F6\nJE2Qu/1SUcOGP4EnI+LZiGi/1lLSVBl2t399Zh6OiN8BfhgRr2bmnsVPaP5T8D8GacoMNfJn5uHm\n71FgF3DtEs/ZlplzfhgoTZeBwx8RqyLiM6fuA18CXhpVY5LGa5jd/hlgV0ScWs+/ZuZ/jKQrSWM3\ncPgz8w3gD0bYi1agVatWtdYvuOCCCXWiM+WpPqkowy8VZfilogy/VJThl4oy/FJR/nS3Wm3cuLG1\nftNNN7XW77zzzoG3/fzzz7fW33zzzYHXLUd+qSzDLxVl+KWiDL9UlOGXijL8UlGGXyrK8/xqdffd\nd7fWN2zY0Fo/efLkwNves2dPa/3VV18deN1y5JfKMvxSUYZfKsrwS0UZfqkowy8VZfilojzPX9zs\n7GxrfWZmprV+zjnjGz+aOSE0Jo78UlGGXyrK8EtFGX6pKMMvFWX4paIMv1RU3/P8EbEduAU4mplX\nNMsuAr4PzAIHgdsz8xfja1ODWr16dWt9+/btrfWrrrqqtd7v+/pt9WPHjrW+dufOna11DWc5I/+3\ngZtPW3Yf8FRmXg481TyWtIL0DX9m7gFOnLZ4I7Cjub8DuG3EfUkas0GP+Wcy80hz/22g/RpQSVNn\n6Gv7MzMjInvVI2IemB92O5JGa9CR/52IWAPQ/D3a64mZuS0z5zJzbsBtSRqDQcO/G9jc3N8MPDaa\ndiRNSt/wR8QjwH8Dvx8RhyLiK8DXgBsj4jXgT5rHklaQyOx5uD76jbV8NqDxePrpp1vr119//VDr\n7/d9/rbz/P1+l/+GG24YqKfqMnNZP4TgFX5SUYZfKsrwS0UZfqkowy8VZfilovzp7rPcmjVrOt3+\nM88807O2adOmCXai0znyS0UZfqkowy8VZfilogy/VJThl4oy/FJRnuc/C9xzzz09axdffPEEO/mk\nXbt29awdP358gp3odI78UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU5/nPAmvXru1ZO//888e67Xff\nfbe1/tZbb411+xqcI79UlOGXijL8UlGGXyrK8EtFGX6pKMMvFdX3PH9EbAduAY5m5hXNsgeAvwCO\nNU+7PzP/fVxNql3bNOttU2SPwt69e1vrjz/++Fi3r8EtZ+T/NnDzEsv/KTOvbG4GX1ph+oY/M/cA\nJybQi6QJGuaY/96IeCEitkfEhSPrSNJEDBr+bwJfAK4EjgBf7/XEiJiPiP0RsX/AbUkag4HCn5nv\nZOZHmXkS+BZwbctzt2XmXGbODdqkpNEbKPwRsXjq1y8DL42mHUmTspxTfY8AXwQ+GxGHgL8FvhgR\nVwIJHAS2jLFHSWPQN/yZeccSix8aQy+SJsgr/KSiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4\npaIMv1SU4ZeKMvxSUYZfKsrwS0U5RfcKMDfX/iNIt95664Q6+aRrrrmmtd7Wmz/r3S1Hfqkowy8V\nZfilogy/VJThl4oy/FJRhl8qyvP8K8Dq1atb67Ozs5NpZAkzMzOt9csuu2xCnehMOfJLRRl+qSjD\nLxVl+KWiDL9UlOGXijL8UlF9z/NHxKXAd4AZIIFtmfnPEXER8H1gFjgI3J6Zvxhfq+rl5MmTnW37\nwIEDrfU9e/ZMqBOdqeWM/B8Cf5WZ64A/BLZGxDrgPuCpzLwceKp5LGmF6Bv+zDySmc81998HXgEu\nATYCO5qn7QBuG1eTkkbvjI75I2IWuArYC8xk5pGm9DYLhwWSVohlX9sfEecDO4GvZuYvI+I3tczM\niMger5sH5odtVNJoLWvkj4hPsRD872bmo83idyJiTVNfAxxd6rWZuS0z5zKz/VcoJU1U3/DHwhD/\nEPBKZn5jUWk3sLm5vxl4bPTtSRqXyFxyb/3/nxCxHvgx8CJw6pzS/Swc9/8b8LvAmyyc6jvRZ13t\nG9OSzjvvvNb6gw8+2LN21113jbqdj3n44Ydb6/PzHvFNWmZG/2ct45g/M/8L6LWyG86kKUnTwyv8\npKIMv1SU4ZeKMvxSUYZfKsrwS0X5090rwAcffNBaf++998a27d27d7fWPY+/cjnyS0UZfqkowy8V\nZfilogy/VJThl4oy/FJRfb/PP9KN+X3+sVi7dm3P2pNPPtn62n379rXWt2zZ0lo/fvx4a12Tt9zv\n8zvyS0UZfqkowy8VZfilogy/VJThl4oy/FJRnueXzjKe55fUyvBLRRl+qSjDLxVl+KWiDL9UlOGX\niuob/oi4NCJ+FBEHIuLliPjLZvkDEXE4In7S3DaMv11Jo9L3Ip+IWAOsycznIuIzwLPAbcDtwK8y\n8x+XvTEv8pHGbrkX+fSdsSczjwBHmvvvR8QrwCXDtSepa2d0zB8Rs8BVwN5m0b0R8UJEbI+IC3u8\nZj4i9kfE/qE6lTRSy762PyLOB54B/j4zH42IGeA4kMDfsXBo8Od91uFuvzRmy93tX1b4I+JTwA+A\nJzLzG0vUZ4EfZOYVfdZj+KUxG9kXeyIigIeAVxYHv/kg8JQvAy+daZOSurOcT/vXAz8GXgRONovv\nB+4ArmRht/8gsKX5cLBtXY780piNdLd/VAy/NH5+n19SK8MvFWX4paIMv1SU4ZeKMvxSUYZfKsrw\nS0UZfqkowy8VZfilogy/VJThl4oy/FJRfX/Ac8SOA28uevzZZtk0mtbeprUvsLdBjbK3y5b7xIl+\nn/8TG4/Yn5lznTXQYlp7m9a+wN4G1VVv7vZLRRl+qaiuw7+t4+23mdbeprUvsLdBddJbp8f8krrT\n9cgvqSOdhD8ibo6In0bE6xFxXxc99BIRByPixWbm4U6nGGumQTsaES8tWnZRRPwwIl5r/i45TVpH\nvU3FzM0tM0t3+t5N24zXE9/tj4hzgZ8BNwKHgH3AHZl5YKKN9BARB4G5zOz8nHBE/BHwK+A7p2ZD\nioh/AE5k5tea/zgvzMy/npLeHuAMZ24eU2+9Zpb+Mzp870Y54/UodDHyXwu8nplvZOavge8BGzvo\nY+pl5h7gxGmLNwI7mvs7WPjHM3E9epsKmXkkM59r7r8PnJpZutP3rqWvTnQR/kuAny96fIjpmvI7\ngScj4tmImO+6mSXMLJoZ6W1gpstmltB35uZJOm1m6al57waZ8XrU/MDvk9Zn5tXAnwJbm93bqZQL\nx2zTdLrmm8AXWJjG7Qjw9S6baWaW3gl8NTN/ubjW5Xu3RF+dvG9dhP8wcOmix59rlk2FzDzc/D0K\n7GLhMGWavHNqktTm79GO+/mNzHwnMz/KzJPAt+jwvWtmlt4JfDczH20Wd/7eLdVXV+9bF+HfB1we\nEZ+PiE8Dm4DdHfTxCRGxqvkghohYBXyJ6Zt9eDewubm/GXisw14+Zlpmbu41szQdv3dTN+N1Zk78\nBmxg4RP//wH+poseevT1e8Dzze3lrnsDHmFhN/B/Wfhs5CvAbwNPAa8B/wlcNEW9/QsLszm/wELQ\n1nTU23oWdulfAH7S3DZ0/d619NXJ++YVflJRfuAnFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmo\n/wN8Y/ZiN7PhuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1733fe3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display random image and label from training data.\n",
    "\n",
    "def random_data_sample(data):\n",
    "    ri = random.randint(0, len(data)-1)\n",
    "    X_rand = data[ri][0]\n",
    "    y_rand = data[ri][1]\n",
    "    \n",
    "    return X_rand, y_rand\n",
    "\n",
    "def display_random_digit(data):\n",
    "    X_rand, y_rand = random_data_sample(data)\n",
    "\n",
    "    rand_img = X_rand.reshape((28, 28))\n",
    "    plt.figure()\n",
    "    plt.imshow(rand_img, cmap='gray')\n",
    "    \n",
    "    print(np.argmax(y_rand))\n",
    "    \n",
    "display_random_digit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAABECAYAAABpjjW9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1733fef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_random_one_hot_encoding(data):\n",
    "    _, y_rand = random_data_sample(data)\n",
    "    print(y_rand)\n",
    "    plt.figure()\n",
    "    plt.imshow(y_rand.T, cmap='gray')\n",
    "    print(np.argmax(y_rand))\n",
    "    \n",
    "display_random_one_hot_encoding(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO display random numbers\n",
    "TODO graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ea650056e551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbiases_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Plo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7a3ffbbabd0e>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(biases, weights, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtest_acc_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mtrain_acc_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-37ae477ca303>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(biases, weights, test_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-37ae477ca303>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((x, y))\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs=3\n",
    "mini_batch_size=10\n",
    "eta=3.0\n",
    "\n",
    "biases_final, weights_final, train_acc, test_acc = SGD(biases, weights, training_data, epochs, mini_batch_size, eta, test_data=test_data)\n",
    "\n",
    "# Plo\n",
    "# plt.plot(range(len(test_acc)), test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.16015625],\n",
      "       [ 0.62890625],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.99609375],\n",
      "       [ 0.76171875],\n",
      "       [ 0.0234375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.44921875],\n",
      "       [ 0.92578125],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.63671875],\n",
      "       [ 0.0234375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.06640625],\n",
      "       [ 0.5390625 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.5859375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.1171875 ],\n",
      "       [ 0.77734375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.97265625],\n",
      "       [ 0.5546875 ],\n",
      "       [ 0.46484375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.78125   ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.25390625],\n",
      "       [ 0.9375    ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9765625 ],\n",
      "       [ 0.42578125],\n",
      "       [ 0.        ],\n",
      "       [ 0.12109375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.50390625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0546875 ],\n",
      "       [ 0.86328125],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.890625  ],\n",
      "       [ 0.37890625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.5       ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.32421875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.09375   ],\n",
      "       [ 0.921875  ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.78125   ],\n",
      "       [ 0.15625   ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.05078125],\n",
      "       [ 0.90234375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.3046875 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.3203125 ],\n",
      "       [ 0.80859375],\n",
      "       [ 0.59375   ],\n",
      "       [ 0.01171875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.91796875],\n",
      "       [ 0.0625    ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0234375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.03125   ],\n",
      "       [ 0.6484375 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.328125  ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.30859375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.93359375],\n",
      "       [ 0.02734375],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0234375 ],\n",
      "       [ 0.78125   ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.98046875],\n",
      "       [ 0.234375  ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.47265625],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.82421875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.171875  ],\n",
      "       [ 0.95703125],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.4921875 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.68359375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.80859375],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.578125  ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.95703125],\n",
      "       [ 0.30078125],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.3046875 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.953125  ],\n",
      "       [ 0.44140625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.19921875],\n",
      "       [ 0.7890625 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.63671875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.01171875],\n",
      "       [ 0.69140625],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.22265625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.35546875],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.9609375 ],\n",
      "       [ 0.19921875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.28515625],\n",
      "       [ 0.77734375],\n",
      "       [ 0.9921875 ],\n",
      "       [ 0.79296875],\n",
      "       [ 0.125     ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ]], dtype=float32), array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.]]))\n"
     ]
    }
   ],
   "source": [
    "for a,b in training_data:\n",
    "    print(a,b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
