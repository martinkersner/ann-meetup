{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>\n",
    "\n",
    "Inspired by http://neuralnetworksanddeeplearning.com/ and https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perceptron\n",
    "* Multilayer perceptron\n",
    "* Backpropagation\n",
    "* Forward pass\n",
    "    * Activation functions\n",
    "* Backward pass\n",
    "    * Stochastic Gradient Descent\n",
    "    * Weights update\n",
    "* MNIST\n",
    "* Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "<img src=\"files/perceptron.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.77]]\n"
     ]
    }
   ],
   "source": [
    "# PERCEPTRON - MATRIX MULTIPLICATION EXAMPLE\n",
    "\n",
    "# weights\n",
    "# w_1 = 1\n",
    "# w_2 = 1.1\n",
    "# w_3 = 0.1\n",
    "# w_4 = 0.2\n",
    "W = np.matrix([[1.0, 1.1, 0.1, 0.2]])\n",
    "\n",
    "# input data\n",
    "# x_1 = 0.5\n",
    "# x_2 = 0.7\n",
    "# x_3 = 3\n",
    "# x_4 = 1.0\n",
    "X = np.matrix([[0.5],\n",
    "               [0.7],\n",
    "               [3],\n",
    "               [1.0]])\n",
    "\n",
    "# bias\n",
    "b = 1\n",
    "\n",
    "# forward pass\n",
    "z = np.dot(W, X) + b\n",
    "print(z)\n",
    "\n",
    "# Activation function\n",
    "# a = activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron\n",
    "\n",
    "<img src=\"files/multi-layer-perceptron.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.05]\n",
      " [ 1.55]\n",
      " [ 3.18]]\n"
     ]
    }
   ],
   "source": [
    "# MULTILAYER PERCEPTRON - MATRIX MULTIPLICATION EXAMPLE\n",
    "\n",
    "# weights\n",
    "# w_11 = 1.0\n",
    "# w_12 = 1.1\n",
    "# w_13 = 1.3\n",
    "# w_21 = 0.1\n",
    "# w_22 = 0.2\n",
    "# w_23 = 0.6\n",
    "# w_31 = 0.4\n",
    "# w_32 = 1.2\n",
    "# w_32 = 1.9\n",
    "W = np.matrix([[1.0, 1.1, 1.3], \n",
    "               [0.1, 0.2, 0.6],\n",
    "               [0.4, 1.2, 1.9]])\n",
    "\n",
    "# input data\n",
    "# x_1 = 0.5\n",
    "# x_2 = 0.7\n",
    "# x_3 = 0.6\n",
    "X = np.matrix([[0.5],\n",
    "               [0.7],\n",
    "               [0.6]])\n",
    "\n",
    "# biases\n",
    "b = np.matrix([[1.0],\n",
    "               [1.0],\n",
    "               [1.0]])\n",
    "\n",
    "\n",
    "# forward pass\n",
    "z = np.dot(W, X) + b\n",
    "print(z)\n",
    "\n",
    "# Activation function\n",
    "# a = activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The biases and weights for the network are initialized randomly, using a Gaussian distribution\n",
    "# with mean 0, and variance 1.\n",
    "def init_biases(layers):\n",
    "     return [np.random.randn(y, 1) for y in layers[1:]]\n",
    "    \n",
    "def init_weights(layers):\n",
    "    return [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "# 3-layer neural network\n",
    "# input layer 10 neurons\n",
    "# hidden layer 15 neurons\n",
    "# output layer 7 neurons\n",
    "layers_tmp = [10, 15, 7]\n",
    "\n",
    "biases_tmp = init_biases(layers_tmp)\n",
    "weights_tmp = init_weights(layers_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# BIASES in layers\n",
    "for b in biases_tmp:\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.62861872],\n",
      "       [ 0.87749937],\n",
      "       [-0.42681781],\n",
      "       [ 2.03876903],\n",
      "       [-0.99044377],\n",
      "       [-1.8378341 ],\n",
      "       [ 0.60901879],\n",
      "       [ 0.94985545],\n",
      "       [ 1.06500423],\n",
      "       [ 2.14882827],\n",
      "       [-0.08968444],\n",
      "       [-2.11499101],\n",
      "       [ 0.67720556],\n",
      "       [-0.83381318],\n",
      "       [ 0.79473084]]), array([[ 1.0134096 ],\n",
      "       [-0.99480337],\n",
      "       [-1.04467281],\n",
      "       [ 1.61890306],\n",
      "       [-1.5788833 ],\n",
      "       [-0.40097283],\n",
      "       [ 0.52063158]])]\n"
     ]
    }
   ],
   "source": [
    "# BIAS VALUES\n",
    "print(biases_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n",
      "(7, 15)\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS in layers\n",
    "for w in weights_tmp:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  1.19566493e+00,   3.07449324e-01,   3.80059953e-03,\n",
      "         -4.50449660e-01,  -2.32557005e-01,  -4.81581169e-02,\n",
      "          4.61037443e-01,  -1.16048675e-01,  -2.41228857e-01,\n",
      "          1.22735572e-01],\n",
      "       [  3.06010916e-01,  -6.24408569e-01,  -1.36339708e+00,\n",
      "          1.08882435e+00,   9.76329491e-01,  -3.58800271e-01,\n",
      "         -1.50973524e-01,  -2.00418311e+00,  -8.72815539e-01,\n",
      "          3.48648709e-01],\n",
      "       [ -3.74901320e-01,  -1.61044721e-01,   9.62709762e-01,\n",
      "         -2.35273354e-01,  -9.68733907e-01,  -1.36116129e+00,\n",
      "          2.30242238e+00,  -7.71952252e-01,  -8.13748324e-01,\n",
      "          1.24040880e-01],\n",
      "       [ -6.64150045e-01,  -7.54327923e-01,   1.09983408e-01,\n",
      "         -1.82816199e-01,   3.48986784e-01,  -1.04337528e-01,\n",
      "          1.90284599e+00,   7.76496857e-01,  -1.73592909e+00,\n",
      "         -2.06154527e-01],\n",
      "       [  3.94573166e-01,  -5.35302732e-01,  -7.37788801e-02,\n",
      "          2.66869979e-02,   5.18065377e-01,  -1.22785259e+00,\n",
      "         -8.81004486e-02,  -4.57949234e-01,   8.01341038e-04,\n",
      "          1.56616096e+00],\n",
      "       [ -8.19726750e-01,   8.13092562e-01,  -2.37373931e-01,\n",
      "         -4.01858212e-01,  -1.86116231e-01,   9.65041580e-01,\n",
      "         -7.28323803e-01,   5.99040427e-01,  -5.07622917e-01,\n",
      "          1.24623414e-01],\n",
      "       [  2.16139349e-01,  -1.16945457e+00,  -5.33515458e-01,\n",
      "         -6.28984368e-01,  -8.18679887e-04,  -5.52158420e-01,\n",
      "          1.20917544e+00,   1.98659324e-01,  -1.77755908e+00,\n",
      "         -1.21186671e+00],\n",
      "       [  3.36341790e-01,  -8.68539721e-01,  -5.63798447e-01,\n",
      "          2.04480630e-01,  -4.20458523e-01,   1.60308435e-01,\n",
      "         -7.10732396e-01,  -1.47897301e+00,   6.88104567e-01,\n",
      "         -1.69500312e-01],\n",
      "       [ -3.77159901e-01,  -7.24829101e-01,  -1.65135598e-01,\n",
      "         -1.76951261e-01,   7.31627078e-01,  -7.29227274e-01,\n",
      "          1.06553718e-01,   1.85055494e+00,   1.00394673e+00,\n",
      "          4.57760425e-01],\n",
      "       [ -3.37210519e-01,  -6.44316147e-01,   1.50371920e-01,\n",
      "         -8.86204850e-01,   7.71696306e-01,   6.94388144e-02,\n",
      "         -1.46073691e-01,  -2.54272026e+00,  -4.47067495e-01,\n",
      "          5.04602501e-01],\n",
      "       [ -5.11338745e-01,  -1.68568087e+00,   3.34793038e-01,\n",
      "         -1.39227040e+00,  -1.27737428e-01,   1.81128444e-02,\n",
      "         -2.70771712e-01,  -2.85972631e-01,  -7.00353264e-01,\n",
      "          2.07160668e+00],\n",
      "       [ -1.95538895e-01,   1.01634735e+00,   4.37319055e-01,\n",
      "         -4.68542028e-01,  -4.36973432e-01,   1.84012673e+00,\n",
      "          7.99867184e-01,   1.39182039e-01,   1.16931266e+00,\n",
      "          4.47586574e-01],\n",
      "       [  1.22315542e-01,  -2.51704325e-01,  -1.18286961e+00,\n",
      "         -2.30263411e-01,  -1.70604293e+00,   4.55687000e-01,\n",
      "          1.17708671e+00,  -5.42591810e-01,   2.91011274e-01,\n",
      "          1.23819588e+00],\n",
      "       [  6.82915438e-01,   6.44942768e-01,   1.37605279e+00,\n",
      "          2.37587771e+00,   1.48447688e+00,  -3.72810132e-01,\n",
      "         -8.13038393e-01,  -3.34543249e-01,   2.80565768e-01,\n",
      "         -1.85819852e+00],\n",
      "       [  3.44017258e-01,   9.25301581e-01,   3.98258201e-01,\n",
      "          3.48366718e-01,  -2.89978822e-01,  -1.32849261e+00,\n",
      "         -5.28505330e-03,   1.89135993e+00,   5.51883207e-02,\n",
      "         -8.33225820e-01]]), array([[ -1.14955555e+00,  -9.27863220e-01,  -4.76297450e-01,\n",
      "         -6.65405999e-01,  -3.31259164e-02,  -9.88052899e-01,\n",
      "         -8.16597826e-01,  -9.91889605e-02,  -1.56200121e+00,\n",
      "          2.32642974e-01,  -1.28794304e+00,   6.79643543e-01,\n",
      "          1.27770166e+00,  -4.93912834e-01,   1.16482422e+00],\n",
      "       [  7.90129944e-01,  -7.98762996e-01,   3.87305939e-03,\n",
      "          5.19644109e-01,   5.30442188e-01,  -1.24718290e+00,\n",
      "          3.66807848e-01,   4.42586117e-01,  -1.29274182e+00,\n",
      "          1.93761798e+00,  -1.50278622e+00,   1.40321206e+00,\n",
      "          1.40560231e+00,  -9.65303083e-02,   4.33954349e-01],\n",
      "       [ -1.31492607e+00,  -5.04265239e-01,  -7.47630753e-01,\n",
      "          9.67945928e-01,   1.27552416e+00,   7.95279289e-01,\n",
      "          6.43108068e-01,  -9.67704761e-01,  -1.01866354e-01,\n",
      "          2.28858748e-01,   6.71474871e-02,  -3.81869039e-01,\n",
      "         -6.73083880e-01,   8.73523388e-01,   2.53417403e+00],\n",
      "       [  1.77155115e-01,   1.27246084e+00,   5.77902723e-01,\n",
      "          6.72445997e-01,   3.38581983e-01,   1.57332006e+00,\n",
      "          1.97308784e-02,   8.20066732e-01,  -9.13492790e-01,\n",
      "          2.42393839e-01,   2.76707817e-01,  -2.53306199e-01,\n",
      "          4.29784622e-01,   9.70170202e-01,   9.42774674e-03],\n",
      "       [  3.20606041e-01,   5.92075151e-01,   1.19958658e+00,\n",
      "         -4.10337503e-01,  -3.35096745e-01,  -2.07455526e-01,\n",
      "          1.18243005e+00,  -1.60087887e+00,   5.88153310e-01,\n",
      "          1.31717761e+00,  -3.03798326e-01,  -7.16928146e-01,\n",
      "         -1.03075201e+00,  -1.30811403e+00,   2.14943747e-01],\n",
      "       [  6.99402562e-01,   1.10439090e+00,   2.37211313e-03,\n",
      "          1.28401767e+00,   6.65388125e-01,  -1.57691893e+00,\n",
      "         -2.58574554e-01,   1.64662779e+00,   1.02552456e+00,\n",
      "          3.39672794e-01,   1.36394169e+00,   1.28756843e+00,\n",
      "         -1.78896904e-01,   1.33433738e+00,   1.25606111e+00],\n",
      "       [ -7.93896661e-01,   4.04666477e-01,  -3.44308045e-01,\n",
      "         -1.25464967e+00,   1.96318870e+00,   3.65756931e-01,\n",
      "         -7.75275388e-01,   1.26621099e+00,   5.51749414e-01,\n",
      "         -4.65792520e-01,   1.58754887e+00,  -6.99324403e-01,\n",
      "         -4.43328735e-02,   8.44951476e-01,   8.72381330e-02]])]\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTS\n",
    "print(weights_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation function\n",
    "\n",
    "* Introduces non-linearity\n",
    "* Saturation problem\n",
    "\n",
    "<img src=\"files/sigmoid.png\" />\n",
    "\n",
    "Other types of activation functions http://cs231n.github.io/neural-networks-1/#actfun:\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* ELU\n",
    "* Maxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6e5c6075d0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e5c707090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function plot\n",
    "x = np.linspace(-7, 7, 80)\n",
    "plt.plot(x, sigmoid(x))\n",
    "\n",
    "# center of sigmoid\n",
    "plt.plot([0,0], [0, 1], c='orange')\n",
    "plt.plot([-7,7], [0.5, 0.5], c='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(biases, weights, a):\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "\n",
    "* Derivative of sigmoid (http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a single training example, forward it through network, computes cost\n",
    "# and compute gradient of weights and biases for each neuron.\n",
    "def backprop(biases, weights, x, y):\n",
    "    # Derivate of sigmoid\n",
    "    def sigmoid_prime(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "    # Cost function\n",
    "    def cost_derivative(output_activations, y):\n",
    "        return (output_activations-y)    \n",
    "    \n",
    "    # gradients of the cost function for each layer\n",
    "    nabla_b = zeros_like(biases)\n",
    "    nabla_w = zeros_like(weights)\n",
    "    \n",
    "    # FORWARD PASS\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b        \n",
    "        activation = sigmoid(z)\n",
    "        \n",
    "        # store all previous z and a vectors\n",
    "        zs.append(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    # BACKWARD PASS\n",
    "    # the last layer\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    # previous layers\n",
    "    # going backwards layer by layer\n",
    "    num_layers = len(weights)+1\n",
    "    for l in xrange(2, num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sp\n",
    "        \n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch\n",
    "\n",
    "Minibatch is randomly selected training inputs of fixed size.\n",
    "\n",
    "## Stochastic gradient descent\n",
    "\n",
    "<img src=\"files/gradient-descent.png\" />\n",
    "\n",
    "<img src=\"files/sgd-update.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update weights.\n",
    "def update_mini_batch(biases, weights, mini_batch, eta):\n",
    "    def update(v, nv, eta, mini_batch_size):\n",
    "        return v-(eta/mini_batch_size)*nv\n",
    "    \n",
    "    def cumulate(nabla_v, delta_nabla_v):\n",
    "        return [nv+dnv for nv, dnv in zip(nabla_v, delta_nabla_v)]\n",
    "        \n",
    "    nabla_b = zeros_like(biases)\n",
    "    nabla_w = zeros_like(weights)\n",
    "    \n",
    "    # Collect weights and bias gradients for each minibatch and sum them\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = backprop(biases, weights, x, y)\n",
    "        nabla_b = cumulate(nabla_b, delta_nabla_b)\n",
    "        nabla_w = cumulate(nabla_w, delta_nabla_w)\n",
    "    \n",
    "    mini_batch_size = len(mini_batch)\n",
    "    \n",
    "    weights = [update(w, nw, eta, mini_batch_size)\n",
    "               for w, nw in zip(weights, nabla_w)]\n",
    "    \n",
    "    biases = [update(b, nb, eta, mini_batch_size)\n",
    "              for b, nb in zip(biases, nabla_b)]\n",
    "    \n",
    "    return biases, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(biases, weights, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \"\"\"Train the neural network using mini-batch stochastic\n",
    "    gradient descent.  The ``training_data`` is a list of tuples\n",
    "    ``(x, y)`` representing the training inputs and the desired\n",
    "    outputs.  The other non-optional parameters are\n",
    "    self-explanatory.  If ``test_data`` is provided then the\n",
    "    network will be evaluated against the test data after each\n",
    "    epoch, and partial progress printed out.  This is useful for\n",
    "    tracking progress, but slows things down substantially.\"\"\"\n",
    "    \n",
    "    def evaluate(biases, weights, data, training_data_evalution=False):\n",
    "        if training_data_evalution:\n",
    "            test_results = [(np.argmax(feedforward(biases, weights, x)), np.argmax(y)) for (x, y) in data]\n",
    "        else:\n",
    "            test_results = [(np.argmax(feedforward(biases, weights, x)), y) for (x, y) in data]\n",
    "\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    if test_data:\n",
    "        n_test = len(test_data)\n",
    "\n",
    "    n = len(training_data)\n",
    "    test_acc_lst = []\n",
    "    train_acc_lst = []\n",
    "    \n",
    "    for j in xrange(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        \n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            biases, weights = update_mini_batch(biases, weights, mini_batch, eta)\n",
    "            \n",
    "        if test_data:\n",
    "            test_acc = evaluate(biases, weights, test_data)\n",
    "            test_acc_lst.append(test_acc/n_test)\n",
    "            \n",
    "            train_acc = evaluate(biases, weights, training_data, training_data_evalution=True)\n",
    "            train_acc_lst.append(train_acc/n)\n",
    "            \n",
    "            print(\"Epoch {0}: {1} / {2}\".format(j, test_acc, n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "            \n",
    "    return biases, weights, train_acc_lst, test_acc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "* http://yann.lecun.com/exdb/mnist/\n",
    "* 28 by 28 pixels\n",
    "* grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e5dc46c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display random image and label from training data.\n",
    "\n",
    "def random_data_sample(data):\n",
    "    ri = random.randint(0, len(data)-1)\n",
    "    X_rand = data[ri][0]\n",
    "    y_rand = data[ri][1]\n",
    "    \n",
    "    return X_rand, y_rand\n",
    "\n",
    "def display_random_digit(data):\n",
    "    X_rand, y_rand = random_data_sample(data)\n",
    "\n",
    "    rand_img = X_rand.reshape((28, 28))\n",
    "    plt.figure()\n",
    "    plt.imshow(rand_img, cmap='gray')\n",
    "    \n",
    "    print(np.argmax(y_rand))\n",
    "    \n",
    "display_random_digit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_random_one_hot_encoding(data):\n",
    "    _, y_rand = random_data_sample(data)\n",
    "    print(y_rand)\n",
    "    plt.figure()\n",
    "    plt.imshow(y_rand.T, cmap='gray')\n",
    "    print(np.argmax(y_rand))\n",
    "    \n",
    "display_random_one_hot_encoding(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluate(biases, weights, test_data):\n",
    "#     \"\"\"Return the number of test inputs for which the neural\n",
    "#     network outputs the correct result. Note that the neural\n",
    "#     network's output is assumed to be the index of whichever\n",
    "#     neuron in the final layer has the highest activation.\"\"\"\n",
    "    \n",
    "#     test_results = [(np.argmax(feedforward(biases, weights, x)), y) for (x, y) in test_data]\n",
    "#     return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO display random numbers\n",
    "TODO graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# The first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron.  \n",
    "layers = [784, 30, 10]\n",
    "biases = init_biases(layers)\n",
    "weights = init_weights(layers)\n",
    "\n",
    "# Training\n",
    "epochs=3\n",
    "mini_batch_size=10\n",
    "eta=3.0\n",
    "\n",
    "biases_final, weights_final, train_acc, test_acc = SGD(biases, weights, training_data, epochs, mini_batch_size, eta, test_data=validation_data)\n",
    "\n",
    "# Plot training curves\n",
    "plt.plot(range(len(test_acc)), test_acc, c=\"r\")\n",
    "plt.plot(range(len(train_acc)), train_acc, c=\"b\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
